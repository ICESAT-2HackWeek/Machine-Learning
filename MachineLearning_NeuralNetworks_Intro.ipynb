{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICESat-2 Hackweek 2020: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yara Mohajerani ([ymohajer@uci.edu](mailto:ymohajer@uci.edu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Before going through this tutorial, make sure you have the correction environment by running\n",
    "\n",
    "`conda create --name <env> --file mlenv.lock`\n",
    "with the `mlenv.lock` file being in this repositoriy, and choose the this environment for your kernel.\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial we will explore the basics of machine learning with an emphasize on neural networks and applications in altimetry. First we will offer a brief introduction to theory and techniques in implementing neural networks, and then will focus on altimetry applications.\n",
    "\n",
    "## Motivation\n",
    "Advances in machine learning have made it a valuable tool for extracting insights from large datasets without the need for developing exact analytical algorithms. In particular, this has proven extremely useful for altimetry applications. For example, imagine you have thousands of satellite images and you want to identify paricular features automatically in the dataset. One way is to manually go through the data, which is not practical. The second way is to come up with an analytical algorithm that uses explicit engineered laws to detect the desired features. This is not very easy or robust, to say the least. But machine learning methods can learn to perform the desired task from the data in an approximate manner. Specifically, *supervised* methods use the data along with the desired outputs to accomplish the learning task. Here we will focus on supervised machine learning. An extremely useful subset of supervised machine learning fields is *neural networks*, which will be discussed in this notebook.\n",
    "\n",
    "Given the shear volume of altimetry data from ICESat-2, the is huge potential in extracting insights from the data with machine learning that were not possible before. Keep in mind that machine learning is a big field and here we focus on only a few specific approaches and a few computational tools. Specifically, we will focus on neural networks largely implemented in `keras`.\n",
    "\n",
    "---\n",
    "Introduction to neural networks\n",
    "---\n",
    "\n",
    "Neural networks use a series of nonlinear transformations with adjustable (trainable) parameters to approximate an input field into a desired output.\n",
    "\n",
    "Each neuron or unit of a network has an associated weight $w$ and bias $b$, and an activation function $f(z)$ for applying a nonlinear transformation such that the output is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(w.x + b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for input $x$.\n",
    "\n",
    "A neural network contains many layers of nodes to accomplish more involved transformations. Note that each unit only has one adjustable bias $b$, but each precedening connected note has a weight $w$ associated with it. All the weighted inputs are summed such that the output is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(b+\\sum_{i} w_ix_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "![Neural network generic example fromw ww.astroml.org](https://www.astroml.org/_images/fig_neural_network_1.png)\n",
    "<sub>Schematic from [AstroML](https://www.astroml.org/book_figures/chapter9/fig_neural_network.html#book-fig-chapter9-fig-neural-network)\n",
    "    \n",
    "    Figure produced by code under BSD license authored by Jake VanderPlas & Brigitta Sipocz.\n",
    "    The figure produced by this code is published in the textbook \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2019)\n",
    "    For more information, see http://astroML.github.com\n",
    "</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of activation functions are \n",
    "Sigmoid:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "plt.plot(x,y)\n",
    "plt.title('Sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n",
    "Rectified Linear Unit (ReLU):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(z) = \\max(0,z)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,101)\n",
    "plt.plot(x[:50],np.zeros(50),'b-')\n",
    "plt.plot(x[50:],x[50:],'b-')\n",
    "plt.title('ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And many others, which is beyond the scope of this brief tutorial. There are many excellent resources on the choice activation functions (e.g. [Neural Networks and Deep Learning by Michael Nielson](http://neuralnetworksanddeeplearning.com/chap3.html))\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Packages like PyTorch and TensforFlow provide the tools to contruct neural networks in Python. However, here we focus on Keras, a higher-level package which makes it easier to contruct a network.\n",
    "\n",
    "Here as an example we will contruct a simple model for the quintessential machine learning example of identifying handwritten digits (MNIST dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Get data and develop neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--  import required packages\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Get built-in MNIST data from keras\n",
    "#-- \"Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\"\"\n",
    "#-- https://keras.io/datasets/#mnist-database-of-handwritten-digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 20, figsize=(16,5))\n",
    "for i in range(20):\n",
    "    ax[i].imshow(x_train[i],cmap='binary')\n",
    "    ax[i].set_title(y_train[i])\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Turn the trainign labels (indicating which number each image is), into one-hot encoding\n",
    "#-- e.g. 3 --> [0,0,0,1,0,0,0,0,0,0]\n",
    "onehot_train = keras.utils.to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Make sequential model\n",
    "model = keras.Sequential()\n",
    "\"\"\"Dense implements the operation: output = activation(dot(input, kernel) + bias)\n",
    "https://keras.io/layers/core/\"\"\"\n",
    "#-- first hidden layer has 64 units, and input is 28*28 which is the flattened input data\n",
    "model.add(Dense(64, activation='relu', input_dim=x_train.shape[1]*x_train.shape[2]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is categorical classification with 10 classes (0-9), hence 10 units in the last layer. Note that the activation function for the last layer is Softmax. Softmax is a normalized exponential function that returns normalized probabilities using\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j}e^{z_j}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $z_i$ is given by $\\sum_j w_{ji}x_j + b_i$.\n",
    "\n",
    "Thus, the neural network provides probabilities for a given input belonging to each of the 10 classes given by each of the 10 output units.\n",
    "\n",
    "We compile the model with a `categorical_crossentropy` loss function used to train the model.\n",
    "\n",
    "Loss functions are also beyond the scope of this short tutorial, but here `categorical_crossentropy` basically takes in onehot encoded data and calculates the cross entropy loss.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " L = \\sum_{c=0}^{M}y_c\\log p_c \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $y_c$ is the truth label for categoriy $c$ (either 0 or 1) and $p_c$ is the prediction for category $c$ (probability between 0 and 1). $M$ is the total number of classes, which is 10 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',     # optimization algorithm used (other examples include scholastic gradient descent, etc)\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) # quantity to be minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can train the model using the `fit` function. Note that we flatten the input images as specified by the input layer when defining our model. `batch_size` refers to the number of images used in each training iteration (every step/iteration that we update the parameters based on the loss function using the optimizer), and `epoch` is total number of times the whole dataset is used. So total number of iterations is `total_size/batch_size * epochs`.\n",
    "\n",
    "We also want to save our model as we train it. One powerfull tool is to create `callbacks` that save the model in after each epoch under specified conditions (for example only saving the best case or all). There are other very useful `callbacks` such as reducing the learning step when we reach a plateau, etc. Here we will just create a model checkpoint to save the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_file = 'mnist_model.h5'\n",
    "#-- create checkpoints that only saves the best trained network so far based on the loss value.\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(chk_file, monitor='loss',\n",
    "                                                     verbose=1, \n",
    "                                                     save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model and pass the checkpoint as a callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Train model\n",
    "#-- We will just use 5 epochs to save time, with batch sizes of 32\n",
    "model.fit(x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]),\n",
    "          onehot_train, epochs=5, batch_size=32, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- turn testing labels to one-hot encoded\n",
    "onehot_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "#-- evaluate performance of model\n",
    "#-- Returns the loss value & metrics value, which in this case is accuracy\n",
    "model.evaluate(x=x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]),\n",
    "               y=onehot_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation output has multiple components. The first is always the loss function, and the remaining elements are the metrics specified in the `compile` command, which in this case is just `accuracy`. We see that even with just 5 epochs we get very good accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "Note we could have also just saved the model after it was trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mnist_model_saveAgain.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a saved file, you can also load it the weights just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regression Exercise with Neural Networks with ATL03-like data\n",
    "---\n",
    "Before breaking into groups, we will do the set up of the problem together.\n",
    "\n",
    "The goal is to now do a regerssion problem. You will have to figure out a way to alter the architecture of the neural network to provide an output that is not a set of classifications, but a regression parameter.\n",
    "\n",
    "First we will create simulated ATL03 data by creating a noisy dataset of a series of patches with linear features and Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- create 2000 line segments in tiles with width 40 and add Guassian noise\n",
    "n_tiles = 2000\n",
    "w = 40\n",
    "pts = 30 # number of poits to sample\n",
    "repeat = 3 # number of times to repeat the noise addition at each coordinate\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(13)\n",
    "\n",
    "#-- training data has dimensions: # of tilesm, # of points, 2 for the x,y coords of points\n",
    "coords = np.empty((n_tiles,pts*repeat,2),dtype=float)\n",
    "#-- we choose a random slope and intercept in the range -4 to 4 for both, with uniform distribution\n",
    "params = np.random.uniform(-4,4,(n_tiles,2))\n",
    "\n",
    "#-- Now populate x with noisy data\n",
    "#-- to make the data more similar to ATL03, we will have multiple y values for each x value\n",
    "for i in range(n_tiles):\n",
    "    #-- create line \n",
    "    line = params[i,0] + np.arange(w)*params[i,1] + np.random.normal(loc=0,scale=5,size=w)\n",
    "    #-- randomly select `pts` number of points\n",
    "    inds = np.random.randint(0,high=w,size=pts)\n",
    "    for r in range(repeat):\n",
    "        coords[i,r*pts:(r+1)*pts,0] = inds\n",
    "        # Add Gaussian noise with standard deviation 10\n",
    "        coords[i,r*pts:(r+1)*pts,1] = line[inds] + np.random.normal(loc=0,scale=10,size=pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Separate 10% of elements for testing\n",
    "#-- randomly extract indices for 10% of elements\n",
    "ii = np.random.randint(0,high=n_tiles,size=int(n_tiles*0.1))\n",
    "x_test = coords[ii]\n",
    "y_test = params[ii]\n",
    "#-- set the rest for training by getting difference between union and intersection\n",
    "jj = np.setdiff1d(np.union1d(np.arange(n_tiles), ii), np.intersect1d(np.arange(n_tiles), ii))\n",
    "x_train = coords[jj]\n",
    "y_train = params[jj]\n",
    "#-- make sure there are no overlapping elements\n",
    "np.intersect1d(ii,jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Plot some example tiles\n",
    "fig,ax = plt.subplots(1, 4,figsize=(16,4))\n",
    "#-- loop over to plot the first 4 tiles\n",
    "for i in range(4):\n",
    "    ax[i].plot(coords[i,:,0],coords[i,:,1],'bo')\n",
    "    ax[i].plot(np.arange(w),params[i,0] + np.arange(w)*params[i,1],'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout (8 minutes)\n",
    "Try to create a neural network that learn from the examples above and get the slope and coefficient for a given linear feature.\n",
    "\n",
    "**HINT** You can use a series of Dense layers as before to find an approximate mapping between input and outputs. However, note that since we want to output regression values here, instead of a `softmax` activation, simply use the `linear` activation to preserve the output of the last layer instead of mapping the output to probability space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### My Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO BE FILLED DURING TUTORIAL AFTER BREAKOUT\n",
    "##\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, in addition to summarizing the model as above, we can also plot it. This is particularly useful as models get larger and more complex with less linear architectures such as skip-connections, parallel layers, etc. that are outside of the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model2,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = keras.callbacks.ModelCheckpoint('regression_model.h5', monitor='loss',\n",
    "                                                     verbose=1, \n",
    "                                                     save_best_only=True)\n",
    "\n",
    "#-- Train model\n",
    "model2.fit(x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]),\\\n",
    "          y_train, epochs=30, batch_size=30, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- evaluate on test data\n",
    "model2.evaluate(x=x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]), y=y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Also evaluate on train data\n",
    "model2.evaluate(x=x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]), y=y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network is able to recover the regression coefficients with high accuracy for the train data and reasonable accuracy for the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlenv]",
   "language": "python",
   "name": "conda-env-mlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
